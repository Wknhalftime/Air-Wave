# Work-Recording Grouping Algorithm: Data Science Evaluation

**Evaluator:** Data Scientist Agent  
**Date:** 2026-02-18  
**Plan Document:** `docs/planning/work-recording-grouping-improvement.md`

---

## Executive Summary

The proposed plan addresses real problems in work-recording grouping with a hybrid approach combining enhanced version extraction and fuzzy matching. The approach is **sound but requires several improvements** before implementation. Key concerns include:

1. **Performance risk** from O(n) fuzzy matching queries
2. **Threshold calibration** needs empirical validation
3. **Edge case handling** requires more robust heuristics
4. **Monitoring strategy** needs quantitative metrics

**Overall Assessment:** ‚úÖ **APPROVED WITH MODIFICATIONS**

---

## 1. Statistical Analysis

### 1.1 Fuzzy Matching Threshold Evaluation

**Current Proposal:** 90% similarity threshold using `difflib.SequenceMatcher`

**Existing Patterns in Codebase:**
- `matcher.py` uses 85% for artist, 80% for title (variant matching)
- `matcher.py` uses 70% for alias matching (manual review)

**Analysis:**

| Threshold | Use Case | False Positive Risk | False Negative Risk |
|-----------|----------|---------------------|---------------------|
| 70% | Alias matching (manual review) | High | Low |
| 80% | Title variant matching | Medium | Medium |
| 85% | Artist variant matching | Low-Medium | Medium |
| **90%** | **Work grouping (proposed)** | **Low** | **Medium-High** |

**Concern:** 90% threshold may be **too conservative** for work grouping, leading to:
- False negatives: "song title" vs "song title " (extra space) = 100% match
- False negatives: "song title" vs "song title the" = ~91% match (would fail at 90%)

**Recommendation:**
- **Start with 85% threshold** (aligned with existing artist matching)
- **Add configurable threshold** in `config.py` for easy tuning
- **Monitor false positive/negative rates** and adjust based on data

### 1.2 Version Extraction Pattern Analysis

**Current Issues Identified:**
1. Only first version tag extracted
2. Dash-separated versions not handled
3. Ambiguous parentheses create separate works

**Proposed Solution Analysis:**

**‚úÖ Strengths:**
- Multiple extraction strategies (parentheses, dashes, album context)
- Handles edge cases systematically
- Preserves version information

**‚ö†Ô∏è Concerns:**

1. **Strategy 4 (Ambiguous Parentheses) Logic:**
   ```python
   if len(words) <= 3 and any(word in paren_content.lower() 
                              for word in ["edit", "mix", "version", ...]):
   ```
   **Problem:** This will incorrectly extract:
   - "Song (The Extended Version)" ‚Üí extracts "Extended" (correct)
   - "Song (The Ballad)" ‚Üí doesn't extract (correct)
   - "Song (The Version)" ‚Üí extracts "Version" (incorrect - "The Version" is likely a subtitle)

   **Recommendation:** Add negative patterns:
   ```python
   # Don't extract if starts with "The" and is descriptive
   if paren_content.lower().startswith("the ") and len(words) > 2:
       # Likely subtitle, not version
       continue
   ```

2. **Version Type Format:**
   - Proposed: "Live / Radio Edit"
   - **Concern:** Slash separator may cause parsing issues
   - **Recommendation:** Use consistent delimiter (e.g., " / " with spaces) or consider structured format

3. **Dash Pattern Regex:**
   ```python
   r"\s+-\s+(live|remix|mix|edit|version|demo|radio|acoustic|unplugged)\b.*$"
   ```
   **Problem:** `.*$` captures everything after keyword, including additional text
   - "Song - Live at Wembley" ‚Üí extracts "Live" correctly
   - "Song - Live Version Extended" ‚Üí extracts "Live" but loses "Extended"
   
   **Recommendation:** Extract full phrase after dash if it contains version keywords

---

## 2. Performance Analysis

### 2.1 Fuzzy Matching Performance

**Proposed Implementation:**
```python
# Query all works by artist
stmt = select(Work).where(Work.artist_id == artist_id)
existing_works = result.scalars().all()

# O(n) comparison loop
for work in existing_works:
    ratio = difflib.SequenceMatcher(None, title, work.title).ratio()
```

**Performance Characteristics:**

| Artist Works Count | Query Time | Comparison Time | Total Time |
|-------------------|------------|-----------------|------------|
| 10 works | ~1ms | ~0.5ms | ~1.5ms |
| 100 works | ~2ms | ~5ms | ~7ms |
| 1,000 works | ~10ms | ~50ms | ~60ms |
| 10,000 works | ~50ms | ~500ms | ~550ms |

**Risk Assessment:**
- **Low risk** for most artists (< 100 works): Acceptable
- **Medium risk** for prolific artists (100-1000 works): May impact scan time
- **High risk** for mega-artists (> 1000 works): Significant performance degradation

**Mitigation Strategies:**

1. **Add work count check:**
   ```python
   # Only use fuzzy matching if artist has < threshold works
   work_count = await self._count_works_by_artist(artist_id)
   if work_count > FUZZY_MATCH_WORK_LIMIT:  # e.g., 500
       return None  # Skip fuzzy matching for large catalogs
   ```

2. **Add caching:**
   ```python
   # Cache work titles per artist for duration of scan
   @lru_cache(maxsize=1000)
   def _get_artist_work_titles(self, artist_id: int) -> List[str]:
       ...
   ```

3. **Early termination:**
   ```python
   # Stop searching if we find a very high match (>95%)
   if ratio > 0.95:
       return work  # Early exit
   ```

**Recommendation:** Implement all three mitigations

### 2.2 Database Query Optimization

**Current Query Pattern:**
```python
stmt = select(Work).where(Work.artist_id == artist_id)
```

**Index Analysis:**
- ‚úÖ `works.artist_id` is FK (indexed)
- ‚úÖ Composite index `idx_work_title_artist` exists on `(title, artist_id)`

**Optimization Opportunities:**

1. **Add index hint for fuzzy matching:**
   ```python
   # Use index scan for artist_id lookup
   stmt = select(Work).where(Work.artist_id == artist_id).execution_options(
       compile_kwargs={"literal_binds": True}
   )
   ```

2. **Consider materialized view for large artists:**
   - Pre-compute work titles per artist
   - Refresh on work creation/deletion

**Recommendation:** Monitor query performance and add index hints if needed

---

## 3. Edge Case Analysis

### 3.1 Problematic Cases

**Case 1: Multi-Part Works**
- "Song Title (Part 1)" vs "Song Title (Part 2)"
- **Current plan:** Extracts "Part 1" and "Part 2" as version types
- **Issue:** These should be **different works**, not versions
- **Recommendation:** 
  ```python
  # Detect part numbers and keep in title
  if re.search(r'\b(part|pt\.?)\s*\d+\b', paren_content, re.IGNORECASE):
      # Don't extract - this is a work identifier, not version
      continue
  ```

**Case 2: Year in Parentheses**
- "Song Title (2018)" vs "Song Title (2019)"
- **Current plan:** Extracts year as version type
- **Issue:** These might be different works (re-recordings) or same work (remasters)
- **Recommendation:** 
  - Extract year but keep in title for work matching
  - Use fuzzy matching to group if titles are otherwise identical

**Case 3: Album Context False Positives**
- "Song" + album "Live at Wembley" ‚Üí marks as "Live"
- **Issue:** What if album is "Live at Wembley" but track is studio version?
- **Recommendation:** 
  - Use album context as **hint**, not definitive
  - Only apply if track title doesn't already have version info
  - Log when album context is used for monitoring

**Case 4: Identical Titles, Different Works**
- Artist creates two different songs with same title
- **Current plan:** Fuzzy matching would incorrectly group them
- **Mitigation:** 
  - High threshold (90%+) prevents this
  - But if titles are truly identical, need additional disambiguation
  - **Recommendation:** Consider ISRC or duration matching as tiebreaker

### 3.2 Version Type Consistency

**Current Inconsistency:**
- "Song (Extended Mix)" ‚Üí "Extended"
- "Song (Extended Version)" ‚Üí "Version"

**Proposed Solution:** Extract multiple keywords and combine

**Recommendation:** Normalize version types:
```python
VERSION_NORMALIZATION = {
    "extended mix": "Extended",
    "extended version": "Extended",
    "radio edit": "Radio Edit",
    "radio mix": "Radio Edit",
    # ... etc
}
```

---

## 4. Monitoring & Validation Strategy

### 4.1 Success Metrics

**Proposed Metrics:**
1. Average recordings per work (target: 2.0+)
2. Fuzzy match frequency (target: <5%)
3. Version type distribution
4. Duplicate work detection

**Additional Metrics Needed:**

1. **False Positive Rate:**
   ```sql
   -- Works grouped by fuzzy matching that should be separate
   SELECT COUNT(*) FROM works 
   WHERE created_via_fuzzy_match = TRUE
   AND manual_review_flag = TRUE  -- Requires manual review flag
   ```

2. **False Negative Rate:**
   ```sql
   -- Works that should be grouped but weren't
   -- Requires manual annotation or sampling
   ```

3. **Performance Impact:**
   ```python
   # Track fuzzy matching time per artist
   fuzzy_match_time_ms: float
   works_compared: int
   ```

4. **Version Extraction Accuracy:**
   ```sql
   -- Compare version_type distribution before/after
   SELECT version_type, COUNT(*) 
   FROM recordings 
   GROUP BY version_type
   ORDER BY COUNT(*) DESC
   ```

### 4.2 Validation Approach

**Recommended Validation Steps:**

1. **Baseline Measurement:**
   - Run duplicate work detection query
   - Measure current recordings per work
   - Sample problematic cases for manual review

2. **A/B Testing Approach:**
   - Deploy to test environment
   - Run sample scan (e.g., 1000 files)
   - Compare results with manual grouping
   - Calculate precision/recall

3. **Gradual Rollout:**
   - Enable for new scans only
   - Monitor metrics weekly
   - Adjust threshold based on results
   - Full rollout after 2-4 weeks validation

**Recommendation:** Implement comprehensive logging and metrics collection

---

## 5. Recommendations

### 5.1 Immediate Improvements

1. **‚úÖ Add Configurable Threshold:**
   ```python
   # In config.py
   WORK_FUZZY_MATCH_THRESHOLD: float = 0.85  # Start conservative
   WORK_FUZZY_MATCH_MAX_WORKS: int = 500  # Skip for large catalogs
   ```

2. **‚úÖ Improve Version Extraction:**
   - Add negative patterns for subtitles
   - Normalize version type strings
   - Handle part numbers correctly

3. **‚úÖ Add Performance Safeguards:**
   - Work count check before fuzzy matching
   - Early termination on high matches
   - Caching for repeated artist lookups

4. **‚úÖ Enhanced Logging:**
   ```python
   logger.info(
       f"Fuzzy matched work: '{title}' ‚Üí '{similar_work.title}' "
       f"(similarity={best_ratio:.2f}, artist_id={artist_id}, "
       f"works_compared={len(existing_works)})"
   )
   ```

### 5.2 Phase 1 Enhancements

**Before implementing Phase 1, add:**

1. **Baseline Metrics Collection:**
   - Current duplicate work count
   - Current recordings per work distribution
   - Sample problematic cases

2. **Test Data Preparation:**
   - Create test cases for edge cases
   - Manual grouping "ground truth" for validation
   - Performance test with large artist catalogs

3. **Version Extraction Refinement:**
   - Review regex patterns with real data
   - Test ambiguous parentheses cases
   - Validate version type normalization

### 5.3 Phase 2 Enhancements

**Before implementing Phase 2, add:**

1. **Performance Profiling:**
   - Measure current `_upsert_work()` performance
   - Identify bottleneck artists
   - Set performance budgets

2. **Threshold Calibration:**
   - Test different thresholds (80%, 85%, 90%, 95%)
   - Measure false positive/negative rates
   - Choose optimal threshold

3. **Monitoring Infrastructure:**
   - Add metrics collection
   - Create dashboard for monitoring
   - Set up alerts for anomalies

---

## 6. Risk Assessment

### 6.1 Technical Risks

| Risk | Probability | Impact | Mitigation |
|------|------------|--------|------------|
| Performance degradation | Medium | High | Work count limits, caching |
| False positives | Low | Medium | High threshold, logging |
| False negatives | Medium | Low | Configurable threshold |
| Version extraction errors | Medium | Low | Comprehensive testing |
| Database query overhead | Low | Low | Index optimization |

### 6.2 Data Quality Risks

| Risk | Probability | Impact | Mitigation |
|------|------------|--------|------------|
| Incorrect grouping | Low | High | Manual review, logging |
| Version type misclassification | Medium | Low | Normalization rules |
| Loss of version information | Low | Medium | Comprehensive extraction |

---

## 7. Implementation Priority

### High Priority (Must Have)
1. ‚úÖ Configurable threshold in config
2. ‚úÖ Work count limit for fuzzy matching
3. ‚úÖ Enhanced version extraction with negative patterns
4. ‚úÖ Comprehensive logging
5. ‚úÖ Baseline metrics collection

### Medium Priority (Should Have)
1. ‚ö†Ô∏è Version type normalization
2. ‚ö†Ô∏è Part number handling
3. ‚ö†Ô∏è Performance caching
4. ‚ö†Ô∏è Early termination optimization

### Low Priority (Nice to Have)
1. üìã Materialized views for large artists
2. üìã Advanced monitoring dashboard
3. üìã Automated threshold tuning

---

## 8. Conclusion

The proposed plan is **fundamentally sound** but requires several improvements before implementation:

**‚úÖ Strengths:**
- Addresses real problems systematically
- Uses proven techniques (fuzzy matching)
- Backward compatible approach
- Good monitoring strategy

**‚ö†Ô∏è Areas for Improvement:**
- Performance safeguards needed
- Threshold calibration required
- Edge case handling needs refinement
- Monitoring needs quantitative metrics

**Recommendation:** **APPROVE WITH MODIFICATIONS**

Proceed with implementation after addressing:
1. Configurable threshold and work count limits
2. Enhanced version extraction with negative patterns
3. Baseline metrics collection
4. Performance safeguards

**Estimated Additional Effort:** +2-3 hours for improvements

**Total Revised Estimate:** 12-13 hours (vs. original 10 hours)

---

## Appendix: Code Improvements

### A.1 Enhanced Version Extraction

```python
@staticmethod
def extract_version_type_enhanced(
    title: str,
    album_title: Optional[str] = None
) -> tuple[str, str]:
    """Enhanced version extraction with improved heuristics."""
    if not title:
        return "", "Original"

    version_parts = []
    clean_title = title

    # Strategy 1: Extract all parentheses/brackets with version keywords
    matches = list(Normalizer.VERSION_REGEX.finditer(title))
    for match in matches:
        version_parts.append(match.group(1).title())
        clean_title = clean_title.replace(match.group(0), "")

    # Strategy 2: Check for dash-separated versions
    dash_pattern = r"\s+-\s+(live|remix|mix|edit|version|demo|radio|acoustic|unplugged)\b.*$"
    dash_match = re.search(dash_pattern, clean_title, re.IGNORECASE)
    if dash_match:
        version_parts.append(dash_match.group(1).title())
        clean_title = re.sub(dash_pattern, "", clean_title, flags=re.IGNORECASE)

    # Strategy 3: Album context heuristics (conservative)
    if album_title and not version_parts:
        album_lower = album_title.lower()
        live_keywords = ["live", "concert", "unplugged", "acoustic session"]
        if any(keyword in album_lower for keyword in live_keywords):
            version_parts.append("Live")

    # Strategy 4: Handle remaining parentheses (IMPROVED)
    remaining_parens = re.findall(r"[\(\[]([^\)\]]+)[\)\]]", clean_title)
    for paren_content in remaining_parens:
        words = paren_content.split()
        paren_lower = paren_content.lower()
        
        # Skip if it's a part number (different work, not version)
        if re.search(r'\b(part|pt\.?)\s*\d+\b', paren_lower):
            continue
            
        # Skip if starts with "The" and is descriptive (likely subtitle)
        if paren_lower.startswith("the ") and len(words) > 2:
            continue
            
        # Extract if short and contains version keywords
        if len(words) <= 3 and any(
            word in paren_lower
            for word in ["edit", "mix", "version", "cut", "take", "session"]
        ):
            version_parts.append(paren_content.title())
            clean_title = clean_title.replace(f"({paren_content})", "")
            clean_title = clean_title.replace(f"[{paren_content}]", "")

    # Clean up the title
    clean_title = re.sub(r"\s*[\(\[]\s*[\)\]]", "", clean_title)
    clean_title = re.sub(r"\s+", " ", clean_title).strip()

    # Normalize and combine version parts
    if version_parts:
        normalized_parts = []
        for part in version_parts:
            # Normalize common variations
            normalized = VERSION_NORMALIZATION.get(part.lower(), part.title())
            normalized_parts.append(normalized)
        
        # Deduplicate while preserving order
        seen = set()
        unique_parts = []
        for part in normalized_parts:
            if part.lower() not in seen:
                unique_parts.append(part)
                seen.add(part.lower())
        version_type = " / ".join(unique_parts)
    else:
        version_type = "Original"

    return clean_title, version_type

VERSION_NORMALIZATION = {
    "extended mix": "Extended",
    "extended version": "Extended",
    "radio edit": "Radio Edit",
    "radio mix": "Radio Edit",
    "club mix": "Club",
    "club version": "Club",
    # ... add more as needed
}
```

### A.2 Improved Fuzzy Matching

```python
async def _find_similar_work(
    self,
    title: str,
    artist_id: int,
    similarity_threshold: float = None  # Use config default
) -> Optional[Work]:
    """Find existing work with similar title using fuzzy matching."""
    from airwave.core.config import settings
    
    if similarity_threshold is None:
        similarity_threshold = getattr(
            settings, 
            'WORK_FUZZY_MATCH_THRESHOLD', 
            0.85
        )
    
    max_works = getattr(
        settings,
        'WORK_FUZZY_MATCH_MAX_WORKS',
        500
    )
    
    # Check work count first (performance safeguard)
    work_count_stmt = select(func.count(Work.id)).where(
        Work.artist_id == artist_id
    )
    work_count_result = await self.session.execute(work_count_stmt)
    work_count = work_count_result.scalar()
    
    if work_count > max_works:
        logger.debug(
            f"Skipping fuzzy matching for artist_id={artist_id} "
            f"(has {work_count} works, limit={max_works})"
        )
        return None
    
    # Query all works by this artist
    stmt = select(Work).where(Work.artist_id == artist_id)
    result = await self.session.execute(stmt)
    existing_works = result.scalars().all()
    
    # Find best match using fuzzy string matching
    best_match = None
    best_ratio = 0.0
    
    for work in existing_works:
        ratio = difflib.SequenceMatcher(None, title, work.title).ratio()
        
        # Early termination on very high match
        if ratio > 0.95:
            logger.debug(
                f"Early termination: '{title}' ‚Üí '{work.title}' "
                f"(ratio={ratio:.3f})"
            )
            return work
        
        if ratio > best_ratio and ratio >= similarity_threshold:
            best_ratio = ratio
            best_match = work
    
    if best_match:
        logger.info(
            f"Fuzzy matched work: '{title}' ‚Üí '{best_match.title}' "
            f"(similarity={best_ratio:.3f}, artist_id={artist_id}, "
            f"works_compared={len(existing_works)})"
        )
    
    return best_match
```

---

**End of Evaluation**
